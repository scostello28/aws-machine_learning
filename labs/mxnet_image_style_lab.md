## Using an MXNet Neural Network to Style Images

#### Lab Overview

Neural networks have been used for many applications throughout the deep learning revolution. In this Lab, you will use the AWS Deep Learning AMI using a GPU instance (p2.xlarge). You will perform neural style transfers - an algorithm for combining the content of one image with the style of another image. This process involves using convolutional neural networks (CNN). The code you will run is implemented in Python using the MXNet deep learning framework. Additionally, you will setup a custom Python script to aggregate GPU performance data and publish it into Amazon CloudWatch. You will then be able to examine the performance and cost associated with the CNN as it runs.

#### Lab Objectives

Upon completion of this Lab you will be able to:
- Perform neural style transfers using the AWS Deep Learning AMI
- Publish GPU metrics to Amazon CloudWatch using a Python script
- Examine GPU performance in Amazon CloudWatch

### Connecting to the Virtual Machine using SSH

#### Introduction

In this Lab Step, you will employ a Secure Shell (SSH) client to connect to a remote Linux server. SSH is a cryptographic network protocol for securing data communication. SSH establishes a secure channel over an unsecured network. Common applications include remote login and remote command execution.

*Note*: If you are connecting to an instance automatically created by the Cloud Academy Lab environment, it can take a few minutes to appear. If the instance is not visible, please refresh after a minute or two until it appears.

#### Instructions

**Downloading the appropriate key pair**

If you are using Linux or macOS, download the Privacy-enhanced Electronic Mail (PEM) key pair automatically generated by the Cloud Academy platform. If you are using Windows, download the PuTTY Private Key (PPK) key pair.

**Locating the virtual machine IP address**

1. In the AWS Management Console, navigate to **Services** > **Compute** > **EC2**
2. Click on **Running Instances**, select the target virtual machine, and locate the *IPv4 Public IP* address. An example IPv4 address number is 52.39.32.5. Copy your virtual machine IP address.

`34.216.63.158`

3. Proceed to the **Connecting using Linux/macOS** or **Connecting using Windows** instructions depending on your local operating system.

**Connecting using Linux/macOS**

Linux distributions and macOS include a SSH client that accepts standard PEM keys. Complete the following steps to connect using the *terminal*.

a. Open your terminal application.
b. Enter the following command and press Enter:

`ssh -i /Path/To/Your/KeyPair.pem AMIUserName@YourIPv4Address`

`ssh -i /Users/sec/Documents/aws/keys/085613397405.pem ubuntu@34.216.63.158`

where the command details are:

- `ssh` initiates the SSH connection
- `-i` specifies the identity file
- `/Path/To/Your/Keypair.pem` specifies the location and name of your key pair. An example location might be `/Home/YourUserName/Downloads/KeyPair.pem`.
- AMIUserName specifies the SSH user:
  - For the Amazon Linux Amazon Machine Image (AMI), a standard SSH user is *ec2-user*.
  - For Ubuntu images, a standard SSH user is *ubuntu*.
  - For CentOS images, a standard SSH user is *centos*.
  - For Debian images, a standard SSH user is *admin*.
  - For Red Hat 6.4 and later images, a standard SSH user is *ec2-user*.
- Your *IPv4Address* is the IPv4 address noted earlier in the instructions.

*Note*: Your SSH client may refuse to start the connection due to key permissions. If you receive a warning that the key pair file is unprotected, you must change the permissions. Enter the following command and try the connection command again:

`chmod 600 /Path/To/Your/KeyPair.pem`

`chmod 600 /Users/sec/Documents/aws/keys/085613397405.pem`

c.  After successfully connecting to the virtual machine, you should reach an EC2 terminal prompt

*Note*: If you receive a warning that the host is unknown, enter y or yes to add the host and complete the connection.

### Using the Neural Network to Style an Image

#### Introduction

In this Lab Step, you will use a neural network to artistically style an image. The network takes two images as input: a style image, and a content image. The neural network uses the style image to apply a style to the content image.

The neural network and training algorithms are implemented in the [MXNet](https://mxnet.incubator.apache.org/) framework. MXNet is an open-source scalable deep learning framework. The Amazon Deep Learning AMI with source code includes examples for MXNet including the *neural-style* network you will use in this Lab. The example is written in the Python programming language.

The EC2 instance in this Lab is equipped with a graphics processing unit (GPU) to speed up the time it takes to produce the artistically styled image.

#### Instructions

1. Change into the MXNet neural-style example directory: `cd src/mxnet/example/neural-style/`
2. List the directory contents: `ls`

There are a few items to pay attention to:

- **nstyle.py**: The main Python script. If you understand Python code, you may view the code. However, the details of the code are outside of the scope of this Lab.
- **model**: A directory including neural network parameters obtained by pre-training. By using a pre-trained model, you avoid hours to days of training the model with training images.
- **input**: A directory containing one sample content image (IMG_4343.jpg) and one sample style image (starry_night.jpg). The content image is the following:

**Image to Style**
![singapore](images/singapore.jpg)

**Style Image**
![Starry Night](images/starry_night.png)

**nstyle.py**

```python
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import find_mxnet
import mxnet as mx
import numpy as np
import importlib
import logging
logging.basicConfig(level=logging.DEBUG)
import argparse
from collections import namedtuple
from skimage import io, transform
from skimage.restoration import denoise_tv_chambolle

CallbackData = namedtuple('CallbackData', field_names=['eps','epoch','img','filename'])

def get_args(arglist=None):
    parser = argparse.ArgumentParser(description='neural style')

    parser.add_argument('--model', type=str, default='vgg19',
                        choices = ['vgg'],
                        help = 'the pretrained model to use')
    parser.add_argument('--content-image', type=str, default='input/IMG_4343.jpg',
                        help='the content image')
    parser.add_argument('--style-image', type=str, default='input/starry_night.jpg',
                        help='the style image')
    parser.add_argument('--stop-eps', type=float, default=.005,
                        help='stop if the relative chanage is less than eps')
    parser.add_argument('--content-weight', type=float, default=10,
                        help='the weight for the content image')
    parser.add_argument('--style-weight', type=float, default=1,
                        help='the weight for the style image')
    parser.add_argument('--tv-weight', type=float, default=1e-2,
                        help='the magtitute on TV loss')
    parser.add_argument('--max-num-epochs', type=int, default=1000,
                        help='the maximal number of training epochs')
    parser.add_argument('--max-long-edge', type=int, default=600,
                            help='resize the content image')
    parser.add_argument('--lr', type=float, default=.001,
                        help='the initial learning rate')
    parser.add_argument('--gpu', type=int, default=0,
                        help='which gpu card to use, -1 means using cpu')
    parser.add_argument('--output_dir', type=str, default='output/',
                        help='the output image')
    parser.add_argument('--save-epochs', type=int, default=50,
                        help='save the output every n epochs')
    parser.add_argument('--remove-noise', type=float, default=.02,
                        help='the magtitute to remove noise')
    parser.add_argument('--lr-sched-delay', type=int, default=75,
                        help='how many epochs between decreasing learning rate')
    parser.add_argument('--lr-sched-factor', type=int, default=0.9,
                        help='factor to decrease learning rate on schedule')

    if arglist is None:
        return parser.parse_args()
    else:
        return parser.parse_args(arglist)


def PreprocessContentImage(path, long_edge):
    img = io.imread(path)
    logging.info("load the content image, size = %s", img.shape[:2])
    factor = float(long_edge) / max(img.shape[:2])
    new_size = (int(img.shape[0] * factor), int(img.shape[1] * factor))
    resized_img = transform.resize(img, new_size)
    sample = np.asarray(resized_img) * 256
    # swap axes to make image from (224, 224, 3) to (3, 224, 224)
    sample = np.swapaxes(sample, 0, 2)
    sample = np.swapaxes(sample, 1, 2)
    # sub mean
    sample[0, :] -= 123.68
    sample[1, :] -= 116.779
    sample[2, :] -= 103.939
    logging.info("resize the content image to %s", new_size)
    return np.resize(sample, (1, 3, sample.shape[1], sample.shape[2]))

def PreprocessStyleImage(path, shape):
    img = io.imread(path)
    resized_img = transform.resize(img, (shape[2], shape[3]))
    sample = np.asarray(resized_img) * 256
    sample = np.swapaxes(sample, 0, 2)
    sample = np.swapaxes(sample, 1, 2)

    sample[0, :] -= 123.68
    sample[1, :] -= 116.779
    sample[2, :] -= 103.939
   return np.resize(sample, (1, 3, sample.shape[1], sample.shape[2]))

def PostprocessImage(img):
   img = np.resize(img, (3, img.shape[2], img.shape[3]))
   img[0, :] += 123.68
   img[1, :] += 116.779
   img[2, :] += 103.939
   img = np.swapaxes(img, 1, 2)
   img = np.swapaxes(img, 0, 2)
   img = np.clip(img, 0, 255)
   return img.astype('uint8')

def SaveImage(img, filename, remove_noise=0.):
   logging.info('save output to %s', filename)
   out = PostprocessImage(img)
   if remove_noise != 0.0:
       out = denoise_tv_chambolle(out, weight=remove_noise, multichannel=True)
   io.imsave(filename, out)

def style_gram_symbol(input_size, style):
   _, output_shapes, _ = style.infer_shape(data=(1, 3, input_size[0], input_size[1]))
   gram_list = []
   grad_scale = []
   for i in range(len(style.list_outputs())):
       shape = output_shapes[i]
       x = mx.sym.Reshape(style[i], target_shape=(int(shape[1]), int(np.prod(shape[2:]))))
       # use fully connected to quickly do dot(x, x^T)
       gram = mx.sym.FullyConnected(x, x, no_bias=True, num_hidden=shape[1])
       gram_list.append(gram)
       grad_scale.append(np.prod(shape[1:]) * shape[1])
   return mx.sym.Group(gram_list), grad_scale


def get_loss(gram, content):
   gram_loss = []
   for i in range(len(gram.list_outputs())):
       gvar = mx.sym.Variable("target_gram_%d" % i)
       gram_loss.append(mx.sym.sum(mx.sym.square(gvar - gram[i])))
   cvar = mx.sym.Variable("target_content")
   content_loss = mx.sym.sum(mx.sym.square(cvar - content))
   return mx.sym.Group(gram_loss), content_loss

def get_tv_grad_executor(img, ctx, tv_weight):
   """create TV gradient executor with input binded on img
   """
   if tv_weight <= 0.0:
       return None
   nchannel = img.shape[1]
   simg = mx.sym.Variable("img")
   skernel = mx.sym.Variable("kernel")
   channels = mx.sym.SliceChannel(simg, num_outputs=nchannel)
   out = mx.sym.Concat(*[
       mx.sym.Convolution(data=channels[i], weight=skernel,
                          num_filter=1,
                          kernel=(3, 3), pad=(1,1),
                          no_bias=True, stride=(1,1))
       for i in range(nchannel)])
   kernel = mx.nd.array(np.array([[0, -1, 0],
                                  [-1, 4, -1],
                                  [0, -1, 0]])
                        .reshape((1, 1, 3, 3)),
                        ctx) / 8.0
   out = out * tv_weight
   return out.bind(ctx, args={"img": img,
                              "kernel": kernel})

def train_nstyle(args, callback=None):
    """Train a neural style network.
    Args are from argparse and control input, output, hyper-parameters.
    callback allows for display of training progress.
    """
    # input
    dev = mx.gpu(args.gpu) if args.gpu >= 0 else mx.cpu()
    content_np = PreprocessContentImage(args.content_image, args.max_long_edge)
    style_np = PreprocessStyleImage(args.style_image, shape=content_np.shape)
    size = content_np.shape[2:]

    # model
    Executor = namedtuple('Executor', ['executor', 'data', 'data_grad'])

    model_module =  importlib.import_module('model_' + args.model)
    style, content = model_module.get_symbol()
    gram, gscale = style_gram_symbol(size, style)
    model_executor = model_module.get_executor(gram, content, size, dev)
    model_executor.data[:] = style_np
    model_executor.executor.forward()
    style_array = []
    for i in range(len(model_executor.style)):
        style_array.append(model_executor.style[i].copyto(mx.cpu()))

    model_executor.data[:] = content_np
    model_executor.executor.forward()
    content_array = model_executor.content.copyto(mx.cpu())

    # delete the executor
    del model_executor

    style_loss, content_loss = get_loss(gram, content)
    model_executor = model_module.get_executor(
        style_loss, content_loss, size, dev)

    grad_array = []
    for i in range(len(style_array)):
        style_array[i].copyto(model_executor.arg_dict["target_gram_%d" % i])
        grad_array.append(mx.nd.ones((1,), dev) * (float(args.style_weight) / gscale[i]))
    grad_array.append(mx.nd.ones((1,), dev) * (float(args.content_weight)))

    print([x.asscalar() for x in grad_array])
    content_array.copyto(model_executor.arg_dict["target_content"])

    # train
    # initialize img with random noise
    img = mx.nd.zeros(content_np.shape, ctx=dev)
    img[:] = mx.rnd.uniform(-0.1, 0.1, img.shape)

    lr = mx.lr_scheduler.FactorScheduler(step=args.lr_sched_delay,
            factor=args.lr_sched_factor)

    optimizer = mx.optimizer.NAG(
        learning_rate = args.lr,
        wd = 0.0001,
        momentum=0.95,
        lr_scheduler = lr)
    optim_state = optimizer.create_state(0, img)

    logging.info('start training arguments %s', args)
    old_img = img.copyto(dev)
    clip_norm = 1 * np.prod(img.shape)
    tv_grad_executor = get_tv_grad_executor(img, dev, args.tv_weight)

    for e in range(args.max_num_epochs):
        img.copyto(model_executor.data)
        model_executor.executor.forward()
        model_executor.executor.backward(grad_array)
        gnorm = mx.nd.norm(model_executor.data_grad).asscalar()
        if gnorm > clip_norm:
            model_executor.data_grad[:] *= clip_norm / gnorm

        if tv_grad_executor is not None:
            tv_grad_executor.forward()
            optimizer.update(0, img,
                             model_executor.data_grad + tv_grad_executor.outputs[0],
                             optim_state)
        else:
            optimizer.update(0, img, model_executor.data_grad, optim_state)
        new_img = img
        eps = (mx.nd.norm(old_img - new_img) / mx.nd.norm(new_img)).asscalar()

        old_img = new_img.copyto(dev)
        logging.info('epoch %d, relative change %f', e, eps)
        if eps < args.stop_eps:
            logging.info('eps < args.stop_eps, training finished')
            break

        if callback:
            cbdata = {
                'eps': eps,
                'epoch': e+1,
            }
        if (e+1) % args.save_epochs == 0:
            outfn = args.output_dir + 'e_'+str(e+1)+'.jpg'
            npimg = new_img.asnumpy()
            SaveImage(npimg, outfn, args.remove_noise)
            if callback:
                cbdata['filename'] = outfn
                cbdata['img'] = npimg
        if callback:
            callback(cbdata)

    final_fn = args.output_dir + '/final.jpg'
    SaveImage(new_img.asnumpy(), final_fn)


if __name__ == "__main__":
    args = get_args()
    train_nstyle(args)
```

3. Enter the following multi-line command to use the neural network to produce an artistically styled image:

```sh
python nstyle.py --content-image input/singapore.jpg \
                 --style-image input/starry_night.jpg \
                 --gpu 0 \
                 --output_dir output/ \
                 --save-epochs 20 \
                 --max-num-epochs 300
```

The options passed into the script have the following meanings:

- `--content-image`: Path to the content image
- `--style-image`: Path to the style image
- `--gpu`: Instructs the script the index of the GPU to use. A value of -1 means to use the CPU.
- `--output_dir`: Directory where the generated images are saved
- `--save-epochs`: Number of epochs/iterations between saving intermediate images. You are able to see how the image evolves by saving more intermediate images.
- `--max-num-epochs`: The maximum number of epochs to develop the image before stopping. More epochs can yield a better resulting image.

It takes a couple of minutes for dependencies and the pre-trained model to be loaded. You can ignore warnings you see about the use of deprecated code. Eventually, epoch relative change messages appear.

The relative change measures progress being made on training the output image, with smaller changes indicating slower progress and a nearly optimized image.

4. Once you see a message about **final.jpg**, list the contents of the **output** directory: `ls output`

The intermediate epoch images are in the directory along with the **final.jpg** image. You will view the images in the next Lab Step.

#### Summary

In this Lab Step, you used the MXNet framework to generate an image that combined the content from one image with the style from another image. The source code utilizes a pre-trained neural network for this task.

Note: Running the same computations on the CPU instead of the GPU took approximately 30 times longer to complete. That is why GPUs are very beneficial for machine learning tasks.

**added image using scp**
`scp -i /Users/sec/Documents/aws/keys/085613397405.pem /Users/sec/Documents/aws/images/singapore.jpg ubuntu@34.216.63.158:/home/ubuntu/src/mxnet/example/neural-style/input`

**Initial Image**
![singapore](images/singapore.jpg)

**100 Epochs**
![e_100](images/singapore-e_100.jpg)

**200 Epochs**
![e_200](images/singapore-e_200.jpg)

**Final Image after 300 Epochs**
![singapore_starry_night](images/singapore-starry-night.jpg)

### Downloading the Images Styled by the Neural Network

#### Introduction

You have generated an image that combines the style and content of two source images. In this Lab Step, you will copy the final image and all intermediate images to your local computer for viewing using a secure copy (SCP) client. Secure copy copies files through a secure channel using SSH.

#### Transferring Files Using the scp Command on Linux/macOS

a. In a **new** terminal, enter the following command to copy the images to your local computer:

`scp -i <SSH_KEY_PEM_FILE> ubuntu@<INSTANCE_IP>:/home/ubuntu/src/mxnet/example/neural-style/output/* output`

`scp -i /Users/sec/Documents/aws/keys/085613397405.pem ubuntu@34.216.63.158:/home/ubuntu/src/mxnet/example/neural-style/output/* /Users/sec/Documents/aws/output`

where you replace `<SSH_KEY_PEM_FILE>` with the path to the SSH key PEM file you used to connect to the Lab instance using SSH, and `<INSTANCE_IP>` with the public IP address of the Lab instance. The public IP of the instance can be retrieved from the EC2 Console.

The command copies all of the images in the instances output directory to an output directory on your local computer.

b. When prompted about the inability to establish the authenticity of the host, type yes to trust the host.

3. Open the first intermediate image named e_20.jpg.

At this early stage of training the image, the content is not recognizable. The distinct brush stroke style is apparent much more prominent. As more an more epochs take place, a better balance is struck between the content and the style.

4. View the evolution of the intermediate images beginning with e_20.jpg and ending with e_300.jpg.

You can clearly see the progression of the content image becoming more visible but with the artistic style applied.

5. Compare the input images with the final image final.jpg below

This demonstration is a fantastic example of the what you can achieve with machine learning!

#### Summary

In this Lab Step, you downloaded the generated intermediate images and final image to your local computer. You observed the progression of the artistically styled image and gained an appreciation for what all of the epochs of training are doing.

### Publishing GPU metrics to Amazon CloudWatch

#### Introduction

You are using a GPU to accelerate the machine learning algorithm used to generate an artistically styled image. For this task and other machine learning algorithms, it is helpful to monitor your GPU utilization and other metrics. For example, you can determine if your GPU is appropriately sized for running the jobs you need to be done, or you can adjust the machine learning algorithm parameters to more efficiently utilize the GPU.

In this Lab Step, you will use a script provided by AWS to transmit GPU metrics from the Lab instance's GPU to CloudWatch for later analysis. The script leverages the GPU's application programming interface (API) to pull the metric values from the GPU. Specifically, the [NVIDIA Management Library (NVML)](https://developer.nvidia.com/nvidia-management-library-nvml) is used. You will run the Python nstyle.py script again to exercise the GPU. The first two instructions will guide you through uploading new images to use, although you can use the previous images again.

#### Instructions

1. If you prefer, take a couple of minutes to find new style and content images to use.

You may want to use a photo of yourself for the content image. You can try searching online for a style image you like. More distinct style images may lead to more satisfying results. Both JPEG and PNG formatted images have been confirmed to work.

Reminder: Don't take more than a few minutes to find images. If you have time remaining at the end of the Lab, you can try other images.

2. If you found new images, use your SCP client to upload the new style and content images to the `/home/ubuntu/src/mxnet/example/neural-style/input` directory.
3. Open a second SSH connection to the Lab instance.
4. In the new SSH shell, enter the following command to download the CloudWatch GPU monitoring script:
`wget https://s3.amazonaws.com/aws-bigdata-blog/artifacts/GPUMonitoring/gpumon.py`

You can briefly view the file contents if you are familiar with Python.

5. Set the region in the script to US West 2:

`sed -i "s/boto3\.client('cloudwatch', region_name=EC2_REGION)/boto3.client('cloudwatch', region_name='us-west-2')/" gpumon.py`

6. Install the NVIDIA Management Library for Python:

`sudo pip install nvidia-ml-py`

7. Start running the GPU monitoring script:

`python gpumon.py`

The command will not show any output while it is running. You can tell it is running because you won't have a shell prompt while the command is running. It will continue to run until you stop it.

8. Switch over to the original SSH shell, and run the following command to start a new image styling job:

```sh
python nstyle.py --content-image input/<CONTENT_IMAGE> \
                 --style-image input/<STYLE_IMAGE> \
                 --gpu 0 \
                 --output_dir output/ \
                 --save-epochs 20 \
                 --max-num-epochs 1000
```

where you replace `<CONTENT_IMAGE>` and `<STYLE_IMAGE>` with the images you uploaded earlier or the pre-loaded images. The `--max-num-epochs` has been increased to allow for more epochs and more GPU activity.

Now the GPU is active and there will be some interesting metrics to observe in CloudWatch.

#### Summary

In this Lab Step, you downloaded a GPU monitoring script that pulls metrics from the GPU attached to your instance and uploads the metrics to Amazon CloudWatch. The metrics come from the NVIDIA Management Library because the GPU attached to the instance is made by NVIDIA. You also started another neural network image styling job to exercise the GPU creating more interesting metrics to inspect in the next Lab Step.

### Inspecting the GPU Metrics in Amazon CloudWatch

#### essIntroduction

This Lab Step will explore the GPU monitoring metrics that are uploaded to Amazon CloudWatch when using the GPU monitoring script. The instance used in this Lab has detailed monitoring enabled so that one-minute resolution monitoring charts are available for the Lab instance. The highest resolution available without detailed monitoring enabled is five minutes.

#### Instructions

1. In the AWS Management Console, navigate to **Services** > **Management Tools** > **CloudWatch** > **Metrics**
2. In the metric namespace area below the chart, select Custom **Namespaces** > **DeepLearningTrain**
  - **DeepLearningTrain** is an arbitrary CloudWatch metric namespace name used by the GPU monitoring script. You can also see that the script uploads **4 Metrics** to CloudWatch.
3. Select the **GPUNumber**, **ImageId**, **InstanceId**, **InstanceType** CloudWatch dimensions card.
  - The GPU monitoring script uses these four dimensions to identify the uploaded metrics.
4. Select all four metrics (**Memory Usage**, **Temperature**, **GPU Usage**, **Power Usage**) and adjust the chart duration to 15 minutes.
  - The metrics are shown with a five-minute resolution by default.
5. To increase the metric resolution, select the Graphed metrics (4) tab and change the Period of each metric to 1 minute.
6. Move your cursor over the chart to see metric values during the period when the GPU was active:

![CloudWatch Metrics](images/cloudwatch_metrics.png)

As an example, in the above image, the **GPU Usage** didn't reach 100 (%). This indicates the GPU wasn't fully utilized and is not underpowered for the computation. Similarly, the **Memory Usage** was well below 100% utilization. In this case, there is available capacity in the GPU. Therefore, there is no obvious need to consider a more powerful GPU instance.

#### Summary

In this Lab Step, you observed the GPU metrics uploaded to Amazon CloudWatch. You adjusted the resolution of the metrics to view the highest resolution data available. You also gained insight into the capacity of the GPU. In the example, you learned that there was available GPU capacity suggesting there is no obvious reason to upgrade to a more powerful GPU for the workload.

If you have more time remaining in your Lab Session, feel free to download your latest generated image and try other content and style images:
