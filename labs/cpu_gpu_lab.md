051588034343.pem
34.213.179.50


## Connecting to the Virtual Machine using SSH

**Introduction**
In this Lab Step, you will employ a Secure Shell (SSH) client to connect to a remote Linux server. SSH is a cryptographic network protocol for securing data communication. SSH establishes a secure channel over an unsecured network. Common applications include remote login and remote command execution.

*Note:* If you are connecting to an instance automatically created by the Cloud Academy Lab environment, it can take a few minutes to appear. If the instance is not visible, please refresh after a minute or two until it appears.



**Instructions**

**Downloading the appropriate key pair**

If you are using Linux or macOS, download the Privacy-enhanced Electronic Mail (PEM) key pair automatically generated by the Cloud Academy platform. If you are using Windows, download the PuTTY Private Key (PPK) key pair. Both the PEM and PPK are available in the Your lab data section in the upper-left corner of this Lab.


Locating the virtual machine IP address
1. In the AWS Management Console, navigate to **Services > Compute > EC2**.

2. Click on **Running Instances**, select the target virtual machine, and locate the *IPv4 Public IP address*.

34.208.162.198

3. Proceed to the **Connecting using Linux/macOS** or **Connecting using Windows** instructions depending on your local operating system.


**Connecting using Linux/macOS**

Linux distributions and macOS include a SSH client that accepts standard PEM keys. Complete the following steps to connect using the included terminal applications:

a. Open your terminal application.
b. Enter the following command and press Enter:

`ssh -i /Path/To/Your/KeyPair.pem AMIUserName@YourIPv4Address`
`ssh -i /Users/sec/Documents/aws/keys/051588034343.pem ubuntu@34.213.179.50`

where the command details are:
- `ssh` initiates the SSH connection
- `-i` specifies the identity file
- `/Path/To/Your/Keypair.pem` specifies the location and name of your key pair.
  - An example location might be `/Home/YourUserName/Downloads/KeyPair.pem`.
- `AMIUserName` specifies the SSH user:
  - For the Amazon Linux Amazon Machine Image (AMI), a standard SSH user is `ec2-user`.
  - For Ubuntu images, a standard SSH user is `ubuntu`.
  - For CentOS images, a standard SSH user is `centos`.
  - For Debian images, a standard SSH user is `admin`.
  - For Red Hat 6.4 and later images, a standard SSH user is `ec2-user`.
- `YourIPv4Address` is the IPv4 address noted earlier in the instructions.

*Note*: Your SSH client may refuse to start the connection due to key permissions. If you receive a warning that the key pair file is unprotected, you must change the permissions. Enter the following command and try the connection command again:

`chmod 600 /Path/To/Your/KeyPair.pem`

*Note*: If you receive a warning that the host is unknown, enter y or yes to add the host and complete the connection.

## Starting a Jupyter Notebook Server

**Introduction**

The Python programming language is the most popular language for data science and machine learning. Jupyter notebooks are a popular way to share data science and machine learning experiments written in Python and other languages. Jupyter notebooks allow you to share documents that include code and visualizations that the users can execute and interact with from their web browser.

A Jupyter notebook server must be running in order to create and run Jupyter notebooks. The AWS Deep Learning AMI that the Lab virtual machine is built from includes the Jupyter notebook server, in addition to many commonly used packages for machine learning. In this Lab Step, you will start a Jupyter notebook server.



**Instructions**
1. In your SSH shell, enter the following command to start the Jupyter notebook server in the background:

`nohup jupyter notebook &`

The `nohup` command, stands for no hangup and allows the Jupyter notebook server to continue running even if your SSH connection is terminated. After a couple seconds a message about writing output for the process to the *nohup.out* file will be displayed:

```
ubuntu@ip-10-0-0-246:~$ nohup: ignoring input and appending output to 'nohup.out'
```

This will allow you to continue to enter commands at the shell prompt.

2. Press enter to move to a clean command prompt, and tail the notebook's log file to watch for when the notebook is ready to connect to:

`tail -f nohup.out`

The notebook is ready when you see The Jupyter Notebook is running at:...

```
[I 16:25:45.834 NotebookApp] The Jupyter Notebook is running at:
[I 16:25:45.834 NotebookApp] http://localhost:8888/?token=83073d19e6587be4c01c132f4801f840dccd573f20b7cdca
```

3. Press ctrl+c to stop tailing the log file.

4. Enter the following to get an authenticated URL for accessing the Jupyter notebook server:

`jupyter notebook list`

```
Currently running servers:
http://localhost:8888/?token=83073d19e6587be4c01c132f4801f840dccd573f20b7cdca :: /home/ubuntu
```

By default, Jupyter notebooks prevent access to anonymous users. After all, you can run arbitrary code through the notebook interface. The **token** URL parameter is one way to authenticate yourself when accessing the notebook server. The **/home/ubuntu** at the end of the command indicates the working directory of the server.



**Summary**

The Jupyter notebook server is now up and running. However, you can't connect to the server because the security group containing the Lab virtual machine doesn't allow access to port 8888. To maintain a secure environment, only port 22 (SSH) is open. You will learn how to access the notebook server in the next Lab Step.


## Forwarding a Virtual Machine Port through an SSH Tunnel

**Introduction**

SSH tunnels allow you to connect to ports on a remote server through the encrypted SSH channel. This allows you to securely connect to ports on the remote server that you otherwise wouldn't be able to because of system firewall, or security group rules. In this Lab Step, you will establish an SSH connection with a tunnel from port 8000 on your local system to port 8888 on the remote server. This tunnel will allow you to connect from your local machine to the Jupyter notebook server.

Different instructions are provided for Linux/macOS, and Windows.

**Instructions**

1. Navigate to the EC2 Management Console and copy the **IPv4 Public IP address** of the Lab instance.

2. Proceed to the **Connecting using Linux/macOS** or **Connecting using Windows** instructions depending on your local operating system.

**Connecting using Linux / macOS**

Linux distributions and macOS include an SSH client that accepts standard PEM keys. Complete the following steps to connect using the included terminal applications:

a. Open your terminal application. If you need assistance finding the terminal application, search for terminal using your operating system's application finder or search commands.

b. Enter the following command and press Enter:

`ssh -i /Path/To/Your/KeyPair.pem ubuntu@YourIPv4Address -L127.0.0.1:8000:127.0.0.1:8888`

`ssh -i /Users/sec/Documents/aws/keys/051588034343.pem ubuntu@34.213.179.50 -L127.0.0.1:8000:127.0.0.1:8888`

where the command details are:
- `ssh` initiates the SSH connection.
- `-i` specifies the identity file.
- `/Path/To/Your/Keypair.pem` specifies the location and name of your key pair.
- `YourIPv4Address` is the IPv4 address noted earlier in the instructions.
- `-L` specifies to bind `127.0.0.1:8000` on your local machine to `127.0.0.1:8888` on the remote machine.

*Note:* Your SSH client may refuse to start the connection due to key permissions. If you receive a warning that the key pair file is unprotected, you must change the permissions. Enter the following command and try the connection command again:

`chmod 600 /Path/To/Your/KeyPair.pem`

c.  After successfully connecting to the virtual machine, you should reach a terminal prompt similar to the one shown in the image below.

*Note:* If you receive a warning that the host is unknown, enter y or yes to add the host and complete the connection.

## Setting Up the CPU vs. GPU Experiment

**Introduction**

You are now able to set up an experiment that will compare the performance of CPUs and graphics processing units (GPUs). The EC2 instance for this Lab is a p2.xlarge instance, which has 4 CPUs and 1 GPU. The GPU is a NVIDIA Tesla K80 containing 2,496 processing cores.

In this Lab Step, you will create a Jupyter notebook with provided Python code to compare the performance of matrix multiplication computations. Matrix multiplications are a core operation in many machine learning algorithms. The provided code uses the [TensorFlow machine learning library](https://www.tensorflow.org/) to implement the matrix multiplications on CPU and GPU.



**Instructions**

1. Return to your initial SSH shell where you started the Jupyter notebook server, and copy the notebook server URL.

*Note:* You can retrieve the URL again by running the `jupyter notebook list` command again.

2. Open a new browser tab, and paste the URL you copied earlier into the address bar:

```
```

3. Replace the port number *8888* with *8000*, and navigate to the URL.

Recall that the SSH tunnel is from port 8888 on the remote server to port 8000 on your local machine. The landing page shows all the files in the working directory.

4. Click on the **New** button above the file listing table, and select **Environment (conda_tensorflow_p36)**.

The Amazon Deep Learning AMI includes several virtual environments for Python to avoid dependency conflicts between packages. The provided code needs an environment with Python 3.6 and TensorFlow available.

5. Paste the following Python code into the cell:

```python
from __future__ import print_function
import time
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as pyplot


def benchmark(devices):
  '''Benchmark each device by computing matrix products'''
  times = {device: [] for device in devices}
  sizes = range(100, 7000, 500)

  for size in sizes:

    print(f"Calculating {size}x{size} matrix product")

    for device in devices:

      shape = (size, size)
      data_type = tf.float32
      with tf.device(device):
        mat1 = tf.random_uniform(shape=shape, minval=0, maxval=1, dtype=data_type)
        mat2 = tf.random_uniform(shape=shape, minval=0, maxval=1, dtype=data_type)
        matmul = tf.matmul(mat1, mat2)

      with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:
        start_time = time.time()
        result = session.run(matmul)
        time_taken = time.time() - start_time
        print(f"{device} took {round(time_taken,2)}s")
        times[device].append(time_taken)

  return times, sizes


def plot_results(devices, sizes, times):
  '''Plot the benchmark results'''
  fig, (ax1, ax2) = pyplot.subplots(2, 1, sharex=True)

  for device in devices:
    ax1.plot(sizes, times[device], 'o-', label=device)
  ax1.set_ylabel('Compute Time')
  ax1.set_title('Device Compute Time vs. Matrix size')
  ax1.legend(devices, loc=2)

  ax2.plot(sizes, np.divide(times[devices[1]], times[devices[0]]), 'o-', label=device)
  ax2.set_ylabel('GPU Speedup')
  ax2.set_xlabel('Matrix size')
  ax2.set_title('GPU Speedup vs. Matrix size')

  pyplot.show()


def experiment():
  '''Run an experiment that compares CPU and GPU device performance'''
  devices = ["/gpu:0", "/cpu:0"]
  times, sizes = benchmark(devices)
  plot_results(devices, sizes, times)

experiment()
```

Don't worry about understanding the details of the code. The code includes three functions:
- `benchmark`: Runs the matrix multiplication benchmark for both the CPU and GPU devices
- `plot_results`: Generates visualizations to easily interpret the benchmark results
- `experiment`: Runs the overall experiment

6. Return to your initial SSH shell, and enter the following command to monitor the status of the Python process associated with your notebook:

```
top -p `pgrep "python"`
```

```
top - 16:32:04 up 11 min,  1 user,  load average: 2.25, 1.57, 0.88
Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu(s): 89.2 us,  2.9 sy,  0.0 ni,  6.2 id,  1.7 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 62880136 total, 59944140 free,  1361972 used,  1574024 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 60877664 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 1930 ubuntu    20   0 84.149g 1.246g 414092 S 356.0  2.1   0:25.09 python
```

The top command is one way to track the CPU (**%CPU**) and memory (**%MEM**) usage of the **python** process that runs the CPU device benchmark.

7. Enter `s1` to tell top to update the statistics every second.

8. Switch to the SSH shell you used for creating a tunnel, and enter the following command to monitor the status of the GPU:

`watch -n 1 nvidia-smi`

```
Every 1.0s: nvidia-smi

Fri Jan  3 16:35:04 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |
| N/A   66C    P0    57W / 149W |  10919MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1930      C   ...naconda3/envs/tensorflow_p36/bin/python 10906MiB |
+-----------------------------------------------------------------------------+
```

The `nvidia-smi` is the NVIDIA system management information tool. You are using it to display information about the GPU including the temperature (**Temp**), power usage (**Pwr**), memory usage (**Memory-Usage**), and GPU utilization (**GPU-Util**). The watch command is used to update the output every second.



9. Position your Jupyter notebook browser tab, and two SSH shells so that you can see all three at once.

This way you will be able to monitor the CPU and GPU as the experiment runs.

**Summary**

In this Lab Step, you set up the CPU versus GPU experiment. The experiment will benchmark the performance of the CPU and GPU at performing matrix multiplication for different sizes of matrices. You also displayed monitoring information for both the CPU and GPU.

## Running the CPU vs. GPU Experiment

**Introduction**
The CPU versus GPU performance experiment is set up. In this Lab Step, you will run the experiment. You will monitor the CPU and GPU devices while the experiment is being performed



**Instructions**
1. In your Jupyter notebook, click the **Run** button to start running the experiment.

The code will start running. It will take around 30 seconds before you see any output. The first time you run the cell it takes significantly longer than following times because the code is being compiled and libraries are being loaded, and the code is loaded onto the GPU. The first output you see will be warning messages. The warnings don't impact the results of the experiment and can be ignored.

2. Once you see **Calculating 100x100 matrix product**, pay attention to changes in the CPU and GPU statistics.

The CPU statistics indicate that all four cores are being utilized to their full ability.


A **%CPU** value of 400 indicates all four cores are being utilized 100% each. Having all CPUs fully utilized indicates that the performance of the computation is limited by the number of CPUs. That is, more CPUs are needed to perform better. This state is referred to as being compute-bound.

On the other hand, the GPU never seems to reach 100% utilization (**GPU-Util**).

This indicates that the GPU has more capacity before the GPU becomes compute-bound. Other changes you will notice is the addition of a **python** process in the lower **Processes** table. The temperature of the GPU increases quite sharply when the computations first start before plateauing. The power fluctuates from a standby level to an elevated level when the GPU is performing a computation.

3. When the experiment completes, observe the two charts displayed in the notebook.

You will analyze the results in the next Lab Step.

4. Re-select the cell with the Python code in it, and run it again.

Watch for any differences in GPU and CPU metrics that you might have missed the first time.

**Summary**

In this Lab Step, you performed an experiment comparing the performance of computations on CPUs and a GPU. You gained some insight into the compute capacity of each device.

## Analyzing the CPU vs. GPU Experiment Results

**Introduction**

To complete the Lab, you will analyze the results of the CPU versus GPU. Because the experiment is based on matrix multiplication computations, a fundamental operation in many machine learning algorithms, you can expect similar trends when using a GPU instead of a CPU when implementing machine learning algorithms.

**Instructions**

1. Focus on the two charts at the bottom of the code cell that look similar to the following image and see what conclusions you can make:

```
Calculating 100x100 matrix product
/gpu:0 took 0.33s
/cpu:0 took 0.45s
Calculating 600x600 matrix product
/gpu:0 took 0.33s
/cpu:0 took 0.48s
Calculating 1100x1100 matrix product
/gpu:0 took 0.35s
/cpu:0 took 0.42s
Calculating 1600x1600 matrix product
/gpu:0 took 0.48s
/cpu:0 took 0.57s
Calculating 2100x2100 matrix product
/gpu:0 took 0.38s
/cpu:0 took 0.97s
Calculating 2600x2600 matrix product
/gpu:0 took 0.39s
/cpu:0 took 1.4s
Calculating 3100x3100 matrix product
/gpu:0 took 0.41s
/cpu:0 took 2.01s
Calculating 3600x3600 matrix product
/gpu:0 took 0.44s
/cpu:0 took 2.73s
Calculating 4100x4100 matrix product
/gpu:0 took 0.6s
/cpu:0 took 3.83s
Calculating 4600x4600 matrix product
/gpu:0 took 0.63s
/cpu:0 took 5.22s
Calculating 5100x5100 matrix product
/gpu:0 took 0.69s
/cpu:0 took 6.95s
Calculating 5600x5600 matrix product
/gpu:0 took 0.73s
/cpu:0 took 9.1s
Calculating 6100x6100 matrix product
/gpu:0 took 0.63s
/cpu:0 took 11.67s
Calculating 6600x6600 matrix product
/gpu:0 took 0.75s
/cpu:0 took 14.59s
Calculating 7100x7100 matrix product
/gpu:0 took 0.87s
/cpu:0 took 18.1s
Calculating 7600x7600 matrix product
/gpu:0 took 0.85s
/cpu:0 took 21.94s
Calculating 8100x8100 matrix product
/gpu:0 took 0.99s
/cpu:0 took 26.42s
Calculating 8600x8600 matrix product
/gpu:0 took 1.1s
/cpu:0 took 31.62s
Calculating 9100x9100 matrix product
/gpu:0 took 1.23s
/cpu:0 took 37.6s
Calculating 9600x9600 matrix product
/gpu:0 took 1.36s
/cpu:0 took 43.48s
```

![CPU vs GPU](images/cpu_gpu_test.png)

The upper graph shows the time it takes for each device to compute matrix products for different sizes of matrices, with the CPU in orange. The lower graph shows the GPU speedup, which is the ratio of the CPU compute time to the GPU compute time.

Here are some conclusions you may have reached:
- The CPU time grows proportional to the size of the matrix squared or cubed. That is, doubling the matrix size results in a 4X to 8X increase in compute time. This follows the growth in the number of computations required to calculate matrix products of different sizes
- The GPU time grows almost linearly with the size of the matrix for the sizes used in the experiment. That is doubling the size doubles the time. This indicates that the GPU hasn't reached capacity. It is able to add more compute cores to complete the computation in much shorter times than a CPU.
- The compute time for matrices of size less than 1000 is similar for GPU and CPU. Sometimes the CPU performs better than GPU for these small sizes. Transfering results from GPU to CPU and initializing GPU programs create some overhead in using a GPU. In general, GPUs excel for large-scale problems.
- The speedup increases with the matrix size. The maximum reached is around 20X. A 20X speedup can mean a compute job finishing in 1 day on CPU, or in 1 hour on GPU. For larger problems, GPUs can offer speedups in the hundreds.


**Summary**

In this Lab Step, you analyzed the results of the experiment and gained an understanding of when GPUs can offer substantial improvements and when they are overkill. For a complete comparison you need to consider the cost of running GPU instances to running CPU-only instances. Usually, if you have problems large enough to be compute-bound, the prince increase of using GPUs is significantly less than the speedup achieved by using GPUs. This results in a reduced overall price.

For very large problems, you can utilize multiple GPUs in a cluster. AWS provides a [machine learning CloudFormation template](https://aws.amazon.com/blogs/compute/distributed-deep-learning-made-easy/) to help with setting up an auto-scaled cluster of GPUs for machine learning tasks.



**Challenge (Optional)**

If you have time remaining in your Lab Session, try to modify the code to see what size of matrix causes the GPU to become compute-bound. Observe how high the temperature reaches while running in a compute-bound state for an extended period.  Also, see what the maximum speedup you can achieve by using the GPU is.
